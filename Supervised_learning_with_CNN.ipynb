{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import simplefilter\n",
    "#simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#from sage.all import primes_first_n\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from sage.all import is_prime\n",
    "import gc\n",
    "import pickle\n",
    "import ast\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # Check divisibility up to the square root of n\n",
    "    for i in np.arange(2, int(np.sqrt(n)) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "PRIME_COLS_BIG = [str(n+1) for n in range(1000) if is_prime(n+1)]\n",
    "LfunctionTypes = ['Artin', 'BMF', 'CMF', 'DIR', 'ECNF', 'ECQ', 'G2Q', 'HMF'] \n",
    "# Does 'NF' really occur? Yes as the Riemann zeta, which we possibly want to remove\n",
    "\n",
    "   \n",
    "def build_lfunctions_df_big():\n",
    "    \"\"\"Creates the rational L-functions data frame from the data in \"\"\"\n",
    "    # Using the file downloaded from Zenodo \n",
    "    filename = 'lfun_rat_withap.txt'\n",
    "    DF = pd.read_table(filename,delimiter=\":\",header='infer', low_memory=False)\n",
    "    BadLtypes =  sorted(list(set(list(DF['instance_types']))))\n",
    "    BadToGood = {}\n",
    "    for badLtype in BadLtypes:\n",
    "        good = []\n",
    "        for Ltype in LfunctionTypes:\n",
    "            if badLtype.count(Ltype) > 0:\n",
    "                good.append(Ltype)\n",
    "        good = tuple(good)\n",
    "        BadToGood[badLtype] = good\n",
    "\n",
    "    def bad_to_good_Ltypes(bad):\n",
    "        return BadToGood[bad]\n",
    "\n",
    "    DF['instance_types'] = DF.apply(lambda x: bad_to_good_Ltypes(x.instance_types), axis=1)\n",
    "    return DF\n",
    "\n",
    "def write_to_int(an_list):\n",
    "    '''Function to convert the an strings to a list of ints, returns column labels and an list'''\n",
    "    an_list = an_list.replace('[','')\n",
    "    an_list = an_list.replace(']','')\n",
    "    an_list = [int(an) for an in an_list.split(',')]\n",
    "    #print('list length is ', len(an_list))\n",
    "    return an_list\n",
    "\n",
    "def write_to_hasse_normalized_primes_big(ap_list, w, d = 1):\n",
    "    '''Function to convert the an strings to a list of normalized floats, returns column labels and an list of primes'''\n",
    "    ap_list = write_to_int(ap_list)\n",
    "    normalized_list = []\n",
    "    for p, ap in zip(PRIME_COLS_BIG, ap_list):\n",
    "        p = int(p)\n",
    "        if not is_prime(p): continue\n",
    "        normalization_quotient = (d*p**(w/2))**(-1)\n",
    "        normalized_list.append(np.float32(round(ap * normalization_quotient, 5)))\n",
    "    return normalized_list\n",
    "\n",
    "\n",
    "def build_hasse_ap_df_big(DF):\n",
    "    DF_new = pd.DataFrame()\n",
    "    for rlf_label in DF.columns:\n",
    "        if rlf_label == 'ap': continue\n",
    "        DF_new[rlf_label] = DF[rlf_label].copy()\n",
    "    DF_new[PRIME_COLUMNS] = [write_to_hasse_normalized_primes_big(a, w, d) for w, a, d in zip(DF['motivic_weight'], DF['ap'], DF['degree'])]\n",
    "    return DF_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # Check divisibility up to the square root of n\n",
    "    for i in np.arange(2, int(np.sqrt(n)) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "NUM_ANS = 1000\n",
    "PRIME_COLUMNS = [str(n+1) for n in range(NUM_ANS) if is_prime(n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    accuracy = correct / y_true.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 11167297796775735125  # Set your desired seed value\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on PRAT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_table('lfun_rat_withap.txt',delimiter=\":\",header='infer', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DF[(DF['primitive'] == True)&(DF['motivic_weight']==1) & (DF['degree']==4) & (DF['order_of_vanishing']<=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ap = build_hasse_ap_df_big(DF)\n",
    "del DF\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "conductors = DF_ap['conductor'].to_numpy().astype(int)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram with 20 bins\n",
    "sns.histplot(conductors, bins=100)\n",
    "\n",
    "plt.title('Distribution of Conductor')\n",
    "plt.xlabel('Conductor')\n",
    "plt.ylabel('Frequency')\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.gca().xaxis.get_major_formatter().set_scientific(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_instance_type_stats(dataframe, column_name, instance_types):\n",
    "    # Flatten the lists in the column and create a Counter\n",
    "    flattened = [item for sublist in dataframe[column_name].apply(ast.literal_eval) for item in list(set(sublist))]\n",
    "    counts = Counter(flattened)\n",
    "\n",
    "    # Print the count for each instance type\n",
    "    for instance in instance_types:\n",
    "        print(f\"{instance}: {counts.get(instance, 0)}\")\n",
    "\n",
    "#possible_instance_types = ['CMF', 'ECQ', 'Artin', 'ECNF', 'BMF', 'HMF', 'DIR', 'G2Q']\n",
    "possible_instance_types = ['ECNF', 'BMF', 'HMF', 'G2Q']\n",
    "\n",
    "# Compute the statistics\n",
    "compute_instance_type_stats(DF_ap, 'instance_types', possible_instance_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train, DF_test = train_test_split(DF_ap, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_instance_type_stats(DF_train, 'instance_types', possible_instance_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test set is evenly distributed\n",
    "compute_instance_type_stats(DF_test, 'instance_types', possible_instance_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lists of indices in the test set for each type\n",
    "type_indices = {\n",
    "    instance_type: DF_test['instance_types'].apply(ast.literal_eval).apply(lambda x: instance_type in x).values\n",
    "    for instance_type in possible_instance_types\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "feature_columns = PRIME_COLUMNS[:168]\n",
    "\n",
    "# Extract feature columns and label\n",
    "X_train = DF_train[feature_columns].values  # feature_columns are your input features\n",
    "y_train = DF_train['order_of_vanishing'].values  # This is your label\n",
    "X_test = DF_test[feature_columns].values  # feature_columns are your input features\n",
    "y_test = DF_test['order_of_vanishing'].values  # This is your label\n",
    "\n",
    "#del DF_train, DF_test, DF_ap\n",
    "#gc.collect()\n",
    "\n",
    "# Scale the data (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN (batch_size, channels, sequence_length)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # 1 channel\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # 1 channel\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "saliency_value_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # Larger kernel size\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=1)\n",
    "\n",
    "        # Dummy input to calculate flattened_size\n",
    "        dummy_input = torch.zeros(1, 1, X_train.shape[-1])  # Example input size (batch_size, channels, length)\n",
    "        self.flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)  # Final output layer for classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling           \n",
    "        return x.view(1, -1).size(1)  # Flatten and get size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = len(set(y_test.numpy()))\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store accuracy values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_accuracies_type = {instance_type : [] for instance_type in possible_instance_types}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Test accuracies for different types\n",
    "    for instance_type in possible_instance_types:\n",
    "        test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "        test_accuracies_type[instance_type].append(test_accuracy) \n",
    "        \n",
    "# Training the model\n",
    "epochs = max_epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train accuracy\n",
    "        #train_outputs = model(X_train)\n",
    "        #train_accuracy = calculate_accuracy(y_train, train_outputs)\n",
    "        train_accuracy = calculate_accuracy(labels, outputs)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test accuracy\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Test accuracies for different types\n",
    "        for instance_type in possible_instance_types:\n",
    "            test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "            test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "            test_accuracies_type[instance_type].append(test_accuracy) \n",
    "\n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_accuracy_list.append(test_accuracies)\n",
    "#torch.save(model.state_dict(), f'Conductor_models/CNN_{conductor_min}_to_{conductor_min}.pth')\n",
    "\n",
    "    \n",
    "# Saliency Map Calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get a single sample from the test set for saliency calculation\n",
    "input_data = X_test[torch.randperm(X_test.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data.to(device))\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "one_hot = torch.zeros_like(output)\n",
    "one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "\n",
    "# Perform a single backward pass for the entire batch\n",
    "output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "# The gradients for input_data will now be populated for the entire batch\n",
    "saliency = input_data.grad  # Saliency map for each sample\n",
    "    \n",
    "# Average the saliency across the training set\n",
    "saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "saliency_value_list.append(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_type_dict = dict.fromkeys(possible_instance_types, None)\n",
    "for instance_type in possible_instance_types:\n",
    "\n",
    "    # Get a single sample from the test set for saliency calculation\n",
    "    X_test_type = X_test[type_indices[instance_type]]\n",
    "    input_data = X_test_type[torch.randperm(X_test_type.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_data.to(device))\n",
    "    _, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "    one_hot = torch.zeros_like(output)\n",
    "    one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "    \n",
    "    # Perform a single backward pass for the entire batch\n",
    "    output.backward(gradient=one_hot, retain_graph=True)\n",
    "    \n",
    "    # The gradients for input_data will now be populated for the entire batch\n",
    "    saliency = input_data.grad  # Saliency map for each sample\n",
    "        \n",
    "    # Average the saliency across the training set\n",
    "    saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    saliency_type_dict[instance_type] = saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.plot(range(1, epochs+1), test_accuracy_list[i], label='Total')\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.plot(range(0, epochs+1), test_accuracies_type[instance_type], label=f'{instance_type}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy over Epochs for Different Types')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "prime_numbers = [float(num) for num in feature_columns]\n",
    "i = 0   \n",
    "plt.scatter(prime_numbers, saliency_value_list[i], label='Total', s=1)\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.scatter(prime_numbers, saliency_type_dict[instance_type], label=f'{instance_type}', s=1)\n",
    "#plt.scatter(prime_numbers, np.log(prime_numbers) / prime_numbers / (np.log(10000)), label='log(p)/p', s=1)\n",
    "plt.title('Saliency Map for Feature Importance')\n",
    "plt.ylabel('Saliency (Gradient Magnitude)')  # Label for x-axis\n",
    "plt.xlabel('p')  # Label for y-axis\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "prime_numbers = [float(num) for num in feature_columns]\n",
    "i = 0\n",
    "plt.scatter(prime_numbers, saliency_value_list[i]/np.max(saliency_value_list[i]), label='Total', s=1)\n",
    "plt.scatter(prime_numbers, np.log(prime_numbers) / prime_numbers / (np.log(2)/2), label='log(p)/p', s=1)\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.scatter(prime_numbers, saliency_type_dict[instance_type]/np.max(saliency_type_dict[instance_type]), \n",
    "                label=f'{instance_type}', s=1)\n",
    "plt.title('Saliency Map for Feature Importance')\n",
    "plt.ylabel('Saliency (normalized by max value)')  # Label for x-axis\n",
    "plt.xlabel('p')  # Label for y-axis\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_type in possible_instance_types:\n",
    "    print(instance_type, f'{test_accuracies_type[instance_type][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning between ECNF and G2Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 11167297796775735125  # Set your desired seed value\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### G2Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_G2Q = DF_ap[DF_ap['instance_types'].apply(ast.literal_eval).apply(lambda x: 'G2Q' in x).values]\n",
    "DF_no_G2Q = DF_ap[~DF_ap['instance_types'].apply(ast.literal_eval).apply(lambda x: 'G2Q' in x).values]\n",
    "DF_train, _ = train_test_split(DF_no_G2Q, test_size=0.2, random_state=0)\n",
    "_, DF_test = train_test_split(DF_G2Q, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4000\n",
    "feature_columns = PRIME_COLUMNS[:168]\n",
    "\n",
    "# Extract feature columns and label\n",
    "X_train = DF_train[feature_columns].values  # feature_columns are your input features\n",
    "y_train = DF_train['order_of_vanishing'].values  # This is your label\n",
    "X_test = DF_test[feature_columns].values  # feature_columns are your input features\n",
    "y_test = DF_test['order_of_vanishing'].values  # This is your label\n",
    "\n",
    "#del DF_train, DF_test, DF_ap\n",
    "#gc.collect()\n",
    "\n",
    "# Scale the data (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN (batch_size, channels, sequence_length)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # 1 channel\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # 1 channel\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "saliency_value_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # Larger kernel size\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=1)\n",
    "\n",
    "        # Dummy input to calculate flattened_size\n",
    "        dummy_input = torch.zeros(1, 1, X_train.shape[-1])  # Example input size (batch_size, channels, length)\n",
    "        self.flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)  # Final output layer for classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling           \n",
    "        return x.view(1, -1).size(1)  # Flatten and get size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = 5\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store accuracy values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  \n",
    "    test_outputs = model(X_test.to(device))\n",
    "    test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "# Training the model\n",
    "epochs = max_epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if epoch == 0 and step == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_accuracy = calculate_accuracy(labels, outputs)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "            model.train()\n",
    "        step += 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train accuracy\n",
    "        #train_outputs = model(X_train)\n",
    "        #train_accuracy = calculate_accuracy(y_train, train_outputs)\n",
    "        train_accuracy = calculate_accuracy(labels, outputs)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test accuracy\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_accuracy_list.append(test_accuracies)\n",
    "#torch.save(model.state_dict(), f'Conductor_models/CNN_{conductor_min}_to_{conductor_min}.pth')\n",
    "\n",
    "    \n",
    "# Saliency Map Calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get a single sample from the test set for saliency calculation\n",
    "input_data = X_test[torch.randperm(X_test.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data.to(device))\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "one_hot = torch.zeros_like(output)\n",
    "one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "\n",
    "# Perform a single backward pass for the entire batch\n",
    "output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "# The gradients for input_data will now be populated for the entire batch\n",
    "saliency = input_data.grad  # Saliency map for each sample\n",
    "    \n",
    "# Average the saliency across the training set\n",
    "saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "saliency_value_list.append(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "print(f'{step_per_epoch} steps per epoch')\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.plot(range(0, epochs+1) , train_accuracies, label='Training with ECNF')\n",
    "plt.plot(range(0, epochs+1) , test_accuracy_list[i], label='Test with G2Q')\n",
    "#plt.plot([epoch*step_per_epoch/10 for epoch in range(1, epochs+1)] , test_accuracy_list[i], label='ECQ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ECNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ECNF = DF_ap[DF_ap['instance_types'].apply(ast.literal_eval).apply(lambda x: 'ECNF' in x).values]\n",
    "DF_no_ECNF = DF_ap[~DF_ap['instance_types'].apply(ast.literal_eval).apply(lambda x: 'ECNF' in x).values]\n",
    "DF_train, _ = train_test_split(DF_no_ECNF, test_size=0.2, random_state=0)\n",
    "_, DF_test = train_test_split(DF_ECNF, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "feature_columns = PRIME_COLUMNS[:168]\n",
    "\n",
    "# Extract feature columns and label\n",
    "X_train = DF_train[feature_columns].values  # feature_columns are your input features\n",
    "y_train = DF_train['order_of_vanishing'].values  # This is your label\n",
    "X_test = DF_test[feature_columns].values  # feature_columns are your input features\n",
    "y_test = DF_test['order_of_vanishing'].values  # This is your label\n",
    "\n",
    "#del DF_train, DF_test, DF_ap\n",
    "#gc.collect()\n",
    "\n",
    "# Scale the data (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN (batch_size, channels, sequence_length)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # 1 channel\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # 1 channel\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "saliency_value_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # Larger kernel size\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=1)\n",
    "\n",
    "        # Dummy input to calculate flattened_size\n",
    "        dummy_input = torch.zeros(1, 1, X_train.shape[-1])  # Example input size (batch_size, channels, length)\n",
    "        self.flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)  # Final output layer for classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling           \n",
    "        return x.view(1, -1).size(1)  # Flatten and get size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = 5\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store accuracy values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  \n",
    "    test_outputs = model(X_test.to(device))\n",
    "    test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "# Training the model\n",
    "epochs = max_epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if epoch == 0 and step == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_accuracy = calculate_accuracy(labels, outputs)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "            model.train()\n",
    "        step += 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train accuracy\n",
    "        #train_outputs = model(X_train)\n",
    "        #train_accuracy = calculate_accuracy(y_train, train_outputs)\n",
    "        train_accuracy = calculate_accuracy(labels, outputs)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test accuracy\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_accuracy_list.append(test_accuracies)\n",
    "#torch.save(model.state_dict(), f'Conductor_models/CNN_{conductor_min}_to_{conductor_min}.pth')\n",
    "\n",
    "    \n",
    "# Saliency Map Calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get a single sample from the test set for saliency calculation\n",
    "input_data = X_test[torch.randperm(X_test.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data.to(device))\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "one_hot = torch.zeros_like(output)\n",
    "one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "\n",
    "# Perform a single backward pass for the entire batch\n",
    "output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "# The gradients for input_data will now be populated for the entire batch\n",
    "saliency = input_data.grad  # Saliency map for each sample\n",
    "    \n",
    "# Average the saliency across the training set\n",
    "saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "saliency_value_list.append(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "print(f'{step_per_epoch} steps per epoch')\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.plot(range(0, epochs+1) , train_accuracies, label='Training with G2Q')\n",
    "plt.plot(range(0, epochs+1) , test_accuracy_list[i], label='Test with ECNF')\n",
    "#plt.plot([epoch*step_per_epoch/10 for epoch in range(1, epochs+1)] , test_accuracy_list[i], label='ECQ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PCA = DF_ap\n",
    "feature_columns = PRIME_COLUMNS\n",
    "X = DF_PCA[feature_columns]\n",
    "y = DF_PCA['label']\n",
    "pca = PCA(n_components=168)\n",
    "principal_components = pca.fit_transform(X)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(len(feature_columns))])\n",
    "pca_df['label'] = y.values\n",
    "\n",
    "col = 'order_of_vanishing'\n",
    "vals = DF_PCA[col].value_counts()\n",
    "\n",
    "color = DF_PCA[col]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], c=color, alpha=0.7, s=10, cmap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(f'2D PCA colored by {col}')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(label=col)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pca_df.merge(DF_PCA[['label', 'instance_types', 'order_of_vanishing']], on='label', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train, DF_test = train_test_split(pca_df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "feature_columns = PRIME_COLUMNS[:168]\n",
    "PC_columns = [f'PC{i+1}' for i in range(len(feature_columns))]\n",
    "\n",
    "# Extract feature columns and label\n",
    "X_train = DF_train[PC_columns].values  # feature_columns are your input features\n",
    "y_train = DF_train['order_of_vanishing'].values  # This is your label\n",
    "X_test = DF_test[PC_columns].values  # feature_columns are your input features\n",
    "y_test = DF_test['order_of_vanishing'].values  # This is your label\n",
    "\n",
    "#del DF_train, DF_test, DF_ap\n",
    "#gc.collect()\n",
    "\n",
    "# Scale the data (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN (batch_size, channels, sequence_length)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # 1 channel\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # 1 channel\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "saliency_value_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # Larger kernel size\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=1)\n",
    "\n",
    "        # Dummy input to calculate flattened_size\n",
    "        dummy_input = torch.zeros(1, 1, X_train.shape[-1])  # Example input size (batch_size, channels, length)\n",
    "        self.flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)  # Final output layer for classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling           \n",
    "        return x.view(1, -1).size(1)  # Flatten and get size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = len(set(y_test.numpy()))\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store accuracy values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_accuracies_type = {instance_type : [] for instance_type in possible_instance_types}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Test accuracies for different types\n",
    "    for instance_type in possible_instance_types:\n",
    "        test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "        test_accuracies_type[instance_type].append(test_accuracy) \n",
    "        \n",
    "# Training the model\n",
    "epochs = max_epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train accuracy\n",
    "        #train_outputs = model(X_train)\n",
    "        #train_accuracy = calculate_accuracy(y_train, train_outputs)\n",
    "        train_accuracy = calculate_accuracy(labels, outputs)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test accuracy\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Test accuracies for different types\n",
    "        for instance_type in possible_instance_types:\n",
    "            test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "            test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "            test_accuracies_type[instance_type].append(test_accuracy) \n",
    "\n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_accuracy_list.append(test_accuracies)\n",
    "#torch.save(model.state_dict(), f'Conductor_models/CNN_{conductor_min}_to_{conductor_min}.pth')\n",
    "\n",
    "    \n",
    "# Saliency Map Calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get a single sample from the test set for saliency calculation\n",
    "input_data = X_test[torch.randperm(X_test.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data.to(device))\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "one_hot = torch.zeros_like(output)\n",
    "one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "\n",
    "# Perform a single backward pass for the entire batch\n",
    "output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "# The gradients for input_data will now be populated for the entire batch\n",
    "saliency = input_data.grad  # Saliency map for each sample\n",
    "    \n",
    "# Average the saliency across the training set\n",
    "saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "saliency_value_list.append(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_type_dict = dict.fromkeys(possible_instance_types, None)\n",
    "for instance_type in possible_instance_types:\n",
    "\n",
    "    # Get a single sample from the test set for saliency calculation\n",
    "    X_test_type = X_test[type_indices[instance_type]]\n",
    "    input_data = X_test_type[torch.randperm(X_test_type.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_data.to(device))\n",
    "    _, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "    one_hot = torch.zeros_like(output)\n",
    "    one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "    \n",
    "    # Perform a single backward pass for the entire batch\n",
    "    output.backward(gradient=one_hot, retain_graph=True)\n",
    "    \n",
    "    # The gradients for input_data will now be populated for the entire batch\n",
    "    saliency = input_data.grad  # Saliency map for each sample\n",
    "        \n",
    "    # Average the saliency across the training set\n",
    "    saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    saliency_type_dict[instance_type] = saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.plot(range(1, epochs+1), test_accuracy_list[i], label='Total')\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.plot(range(0, epochs+1), test_accuracies_type[instance_type], label=f'{instance_type}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy over Epochs for Different Types (Trained with all PCs)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "prime_numbers = [float(num) for num in feature_columns]\n",
    "i = 0   \n",
    "plt.scatter(prime_numbers[:10], saliency_value_list[i][:10], label='Total', s=3)\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.scatter(prime_numbers[:10], saliency_type_dict[instance_type][:10], label=f'{instance_type}', s=3)\n",
    "#plt.scatter(prime_numbers, np.log(prime_numbers) / prime_numbers / (np.log(10000)), label='log(p)/p', s=1)\n",
    "plt.title('Saliency Map for Feature Importance')\n",
    "plt.ylabel('Saliency (Gradient Magnitude)')  # Label for x-axis\n",
    "plt.xlabel('p')  # Label for y-axis\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "prime_numbers = [float(num) for num in feature_columns]\n",
    "i = 0\n",
    "plt.scatter(prime_numbers[:10], saliency_value_list[i][:10]/np.max(saliency_value_list[i][:10]), label='Total', s=5)\n",
    "#plt.scatter(prime_numbers, np.log(prime_numbers) / prime_numbers / (np.log(2)/2), label='log(p)/p', s=1)\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.scatter(prime_numbers[:10], saliency_type_dict[instance_type][:10]/np.max(saliency_type_dict[instance_type][:10]), \n",
    "                label=f'{instance_type}', s=5)\n",
    "plt.title('Saliency Map for Feature Importance')\n",
    "plt.ylabel('Saliency (normalized by max value)')  # Label for x-axis\n",
    "plt.xlabel('p')  # Label for y-axis\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_type in possible_instance_types:\n",
    "    print(instance_type, f'{test_accuracies_type[instance_type][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained with PC1 and PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "feature_columns = PRIME_COLUMNS[:2]\n",
    "PC_columns = [f'PC{i+1}' for i in range(len(feature_columns))]\n",
    "\n",
    "# Extract feature columns and label\n",
    "X_train = DF_train[PC_columns].values  # feature_columns are your input features\n",
    "y_train = DF_train['order_of_vanishing'].values  # This is your label\n",
    "X_test = DF_test[PC_columns].values  # feature_columns are your input features\n",
    "y_test = DF_test['order_of_vanishing'].values  # This is your label\n",
    "\n",
    "#del DF_train, DF_test, DF_ap\n",
    "#gc.collect()\n",
    "\n",
    "# Scale the data (optional but recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN (batch_size, channels, sequence_length)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # 1 channel\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # 1 channel\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "saliency_value_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # Larger kernel size\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=1)\n",
    "\n",
    "        # Dummy input to calculate flattened_size\n",
    "        dummy_input = torch.zeros(1, 1, X_train.shape[-1])  # Example input size (batch_size, channels, length)\n",
    "        self.flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)  # Final output layer for classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling           \n",
    "        return x.view(1, -1).size(1)  # Flatten and get size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # First conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Second conv layer + pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Third conv layer + pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = len(set(y_test.numpy()))\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store accuracy values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_accuracies_type = {instance_type : [] for instance_type in possible_instance_types}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Test accuracies for different types\n",
    "    for instance_type in possible_instance_types:\n",
    "        test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "        test_accuracies_type[instance_type].append(test_accuracy) \n",
    "        \n",
    "# Training the model\n",
    "epochs = max_epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Train accuracy\n",
    "        #train_outputs = model(X_train)\n",
    "        #train_accuracy = calculate_accuracy(y_train, train_outputs)\n",
    "        train_accuracy = calculate_accuracy(labels, outputs)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test accuracy\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_accuracy = calculate_accuracy(y_test.to(device), test_outputs)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Test accuracies for different types\n",
    "        for instance_type in possible_instance_types:\n",
    "            test_outputs = model(X_test[type_indices[instance_type]].to(device))\n",
    "            test_accuracy = calculate_accuracy(y_test[type_indices[instance_type]].to(device), test_outputs)\n",
    "            test_accuracies_type[instance_type].append(test_accuracy) \n",
    "\n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_accuracy_list.append(test_accuracies)\n",
    "#torch.save(model.state_dict(), f'Conductor_models/CNN_{conductor_min}_to_{conductor_min}.pth')\n",
    "\n",
    "    \n",
    "# Saliency Map Calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get a single sample from the test set for saliency calculation\n",
    "input_data = X_test[torch.randperm(X_test.size(0))[:3000]].clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data.to(device))\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# Create a one-hot encoding of the predicted classes (this is a binary mask)\n",
    "one_hot = torch.zeros_like(output)\n",
    "one_hot[torch.arange(output.size(0)), predicted_class] = 1\n",
    "\n",
    "# Perform a single backward pass for the entire batch\n",
    "output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "# The gradients for input_data will now be populated for the entire batch\n",
    "saliency = input_data.grad  # Saliency map for each sample\n",
    "    \n",
    "# Average the saliency across the training set\n",
    "saliency = saliency.abs().mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "saliency_value_list.append(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_\n",
    "\n",
    "# Extract PC1 and PC2 weights\n",
    "pc1_loadings = loadings[0]\n",
    "pc2_loadings = loadings[1]\n",
    "#pc3_loadings = loadings[2]\n",
    "\n",
    "# Feature indices for plotting\n",
    "feature_indices = np.arange(168)\n",
    "\n",
    "# Plot PC1 weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(feature_indices, pc1_loadings, alpha=0.7, label=\"PC1 Loadings\",s=3)\n",
    "plt.scatter(feature_indices, pc2_loadings, alpha=0.7, label=\"PC2 Loadings\", s=3)\n",
    "#plt.scatter(feature_indices, pc3_loadings, alpha=0.7, label=\"PC3 Loadings\", s=2)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Loading Value\")\n",
    "plt.title(\"PCA Loadings for PC1 and PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.plot(range(1, epochs+1), test_accuracy_list[i], label='Total')\n",
    "for instance_type in possible_instance_types:\n",
    "    plt.plot(range(0, epochs+1), test_accuracies_type[instance_type], label=f'{instance_type}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy over Epochs for Different Types (Trained with PC1 and PC2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_type in possible_instance_types:\n",
    "    print(instance_type, f'{test_accuracies_type[instance_type][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
